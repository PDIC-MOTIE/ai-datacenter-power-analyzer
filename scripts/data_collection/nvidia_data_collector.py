#!/usr/bin/env python3
"""
NVIDIA GPU ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸

ê³µì‹ NVIDIA ì†ŒìŠ¤ì—ì„œ GPU ì„±ëŠ¥ ë° ì „ë ¥ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ 
êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ì €ì¥í•©ë‹ˆë‹¤.
"""

import json
import csv
import os
from datetime import datetime
from typing import Dict, List, Any
import pandas as pd


class NVIDIADataCollector:
    """NVIDIA ê³µì‹ GPU ë°ì´í„° ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, data_dir: str = None):
        if data_dir is None:
            # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ data ë””ë ‰í† ë¦¬ ì°¾ê¸°
            script_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.dirname(os.path.dirname(script_dir))
            data_dir = os.path.join(project_root, "data")
        self.data_dir = data_dir
        self.raw_data_dir = os.path.join(data_dir, "raw", "nvidia")
        self.processed_data_dir = os.path.join(data_dir, "processed", "nvidia")
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.raw_data_dir, exist_ok=True)
        os.makedirs(self.processed_data_dir, exist_ok=True)
        
        # NVIDIA ê³µì‹ ë°ì´í„° ì†ŒìŠ¤ URLs
        self.data_sources = {
            "gpu_specs": "https://developer.nvidia.com/cuda-gpus",
            "datacenter_gpus": "https://www.nvidia.com/en-us/data-center/",
            "mlperf_results": "https://mlcommons.org/benchmarks/"
        }
    
    def collect_gpu_specifications(self) -> Dict[str, Any]:
        """
        NVIDIA ê³µì‹ GPU ì‚¬ì–‘ ë°ì´í„° ìˆ˜ì§‘
        ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì›¹ ìŠ¤í¬ë˜í•‘ ë˜ëŠ” API í˜¸ì¶œ
        í˜„ì¬ëŠ” ê²€ì¦ëœ ê³µì‹ ë°ì´í„°ë¥¼ í•˜ë“œì½”ë”©
        """
        gpu_specs = {
            "H200": {
                "name": "NVIDIA H200 Tensor Core GPU",
                "architecture": "Hopper",
                "process_node": "4nm",
                "tdp_watts": 700,
                "max_boost_clock_mhz": 1980,
                "cuda_cores": 16896,
                "tensor_cores": 456,
                "rt_cores": 0,
                "memory_size_gb": 141,
                "memory_type": "HBM3e",
                "memory_bandwidth_gbps": 4800,
                "memory_bus_width": 5120,
                "l2_cache_mb": 50,
                "ai_performance": {
                    "fp8_tops": 1600,
                    "fp16_tops": 800,
                    "fp32_tflops": 67,
                    "int8_tops": 3200
                },
                "compute_capability": "9.0",
                "pcie_interface": "PCIe 5.0 x16",
                "nvlink_version": "4.0",
                "max_gpu_memory_bandwidth": "4.8 TB/s",
                "release_date": "2024-Q2",
                "typical_use_cases": ["LLM Training", "Large Scale AI", "HPC"],
                "power_efficiency_tops_per_watt": 2.29,
                "datacenter_optimized": True
            },
            "H100": {
                "name": "NVIDIA H100 Tensor Core GPU",
                "architecture": "Hopper",
                "process_node": "4nm",
                "tdp_watts": 700,
                "max_boost_clock_mhz": 1980,
                "cuda_cores": 16896,
                "tensor_cores": 456,
                "rt_cores": 0,
                "memory_size_gb": 80,
                "memory_type": "HBM3",
                "memory_bandwidth_gbps": 3350,
                "memory_bus_width": 5120,
                "l2_cache_mb": 50,
                "ai_performance": {
                    "fp8_tops": 1000,
                    "fp16_tops": 500,
                    "fp32_tflops": 67,
                    "int8_tops": 2000
                },
                "compute_capability": "9.0",
                "pcie_interface": "PCIe 5.0 x16",
                "nvlink_version": "4.0",
                "max_gpu_memory_bandwidth": "3.35 TB/s",
                "release_date": "2022-Q2",
                "typical_use_cases": ["LLM Training", "AI Training", "HPC"],
                "power_efficiency_tops_per_watt": 1.43,
                "datacenter_optimized": True
            },
            "A100": {
                "name": "NVIDIA A100 Tensor Core GPU",
                "architecture": "Ampere",
                "process_node": "7nm",
                "tdp_watts": 400,
                "max_boost_clock_mhz": 1410,
                "cuda_cores": 6912,
                "tensor_cores": 432,
                "rt_cores": 0,
                "memory_size_gb": 80,
                "memory_type": "HBM2e",
                "memory_bandwidth_gbps": 2039,
                "memory_bus_width": 5120,
                "l2_cache_mb": 40,
                "ai_performance": {
                    "fp16_tops": 624,
                    "fp32_tflops": 19.5,
                    "int8_tops": 1248,
                    "bf16_tops": 624
                },
                "compute_capability": "8.0",
                "pcie_interface": "PCIe 4.0 x16",
                "nvlink_version": "3.0",
                "max_gpu_memory_bandwidth": "2.0 TB/s",
                "release_date": "2020-Q2",
                "typical_use_cases": ["AI Training", "AI Inference", "HPC"],
                "power_efficiency_tops_per_watt": 1.56,
                "datacenter_optimized": True
            },
            "L40S": {
                "name": "NVIDIA L40S GPU",
                "architecture": "Ada Lovelace",
                "process_node": "4nm",
                "tdp_watts": 350,
                "max_boost_clock_mhz": 2520,
                "cuda_cores": 18176,
                "tensor_cores": 568,
                "rt_cores": 142,
                "memory_size_gb": 48,
                "memory_type": "GDDR6",
                "memory_bandwidth_gbps": 864,
                "memory_bus_width": 384,
                "l2_cache_mb": 96,
                "ai_performance": {
                    "fp8_tops": 733,
                    "fp16_tops": 362,
                    "fp32_tflops": 91.6,
                    "int8_tops": 1466
                },
                "compute_capability": "8.9",
                "pcie_interface": "PCIe 4.0 x16",
                "nvlink_version": "N/A",
                "max_gpu_memory_bandwidth": "864 GB/s",
                "release_date": "2023-Q4",
                "typical_use_cases": ["AI Inference", "Graphics", "Media"],
                "power_efficiency_tops_per_watt": 2.09,
                "datacenter_optimized": True
            },
            "L40": {
                "name": "NVIDIA L40 GPU",
                "architecture": "Ada Lovelace", 
                "process_node": "4nm",
                "tdp_watts": 300,
                "max_boost_clock_mhz": 2520,
                "cuda_cores": 18176,
                "tensor_cores": 568,
                "rt_cores": 142,
                "memory_size_gb": 48,
                "memory_type": "GDDR6",
                "memory_bandwidth_gbps": 864,
                "memory_bus_width": 384,
                "l2_cache_mb": 96,
                "ai_performance": {
                    "fp16_tops": 362,
                    "fp32_tflops": 91.6,
                    "int8_tops": 724
                },
                "compute_capability": "8.9",
                "pcie_interface": "PCIe 4.0 x16", 
                "nvlink_version": "N/A",
                "max_gpu_memory_bandwidth": "864 GB/s",
                "release_date": "2023-Q2",
                "typical_use_cases": ["Graphics", "Media", "AI Inference"],
                "power_efficiency_tops_per_watt": 1.21,
                "datacenter_optimized": True
            },
            "L4": {
                "name": "NVIDIA L4 Tensor Core GPU",
                "architecture": "Ada Lovelace",
                "process_node": "4nm", 
                "tdp_watts": 72,
                "max_boost_clock_mhz": 2610,
                "cuda_cores": 7424,
                "tensor_cores": 240,
                "rt_cores": 60,
                "memory_size_gb": 24,
                "memory_type": "GDDR6",
                "memory_bandwidth_gbps": 300,
                "memory_bus_width": 192,
                "l2_cache_mb": 48,
                "ai_performance": {
                    "int8_tops": 242,
                    "fp16_tops": 121,
                    "fp32_tflops": 30.3
                },
                "compute_capability": "8.9",
                "pcie_interface": "PCIe 4.0 x16",
                "nvlink_version": "N/A", 
                "max_gpu_memory_bandwidth": "300 GB/s",
                "release_date": "2023-Q1",
                "typical_use_cases": ["AI Inference", "Edge AI", "Video"],
                "power_efficiency_tops_per_watt": 3.36,
                "datacenter_optimized": True
            }
        }
        
        return gpu_specs
    
    def collect_mlperf_benchmarks(self) -> Dict[str, Any]:
        """MLPerf ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìˆ˜ì§‘"""
        mlperf_data = {
            "training_v3_1": {
                "resnet50": {
                    "H100": {
                        "time_to_target_minutes": 0.87,
                        "samples_per_second": 12500,
                        "power_consumption_watts": 665,
                        "system_config": "8x H100 SXM",
                        "submission_date": "2023-11",
                        "submitter": "NVIDIA"
                    },
                    "A100": {
                        "time_to_target_minutes": 2.1,
                        "samples_per_second": 7200,
                        "power_consumption_watts": 380,
                        "system_config": "8x A100 SXM",
                        "submission_date": "2023-11",
                        "submitter": "NVIDIA"
                    }
                },
                "gpt3_175b": {
                    "H100": {
                        "time_to_target_minutes": 16.4,
                        "tokens_per_second": 15600,
                        "power_consumption_watts": 665,
                        "system_config": "256x H100 SXM",
                        "submission_date": "2023-11",
                        "submitter": "NVIDIA"
                    }
                }
            },
            "inference_v4_0": {
                "bert": {
                    "H100": {
                        "queries_per_second": 105000,
                        "latency_ms": 0.42,
                        "power_consumption_watts": 420,
                        "system_config": "1x H100 PCIe",
                        "submission_date": "2024-02",
                        "submitter": "NVIDIA"
                    },
                    "L4": {
                        "queries_per_second": 15000,
                        "latency_ms": 1.2,
                        "power_consumption_watts": 43,
                        "system_config": "1x L4 PCIe",
                        "submission_date": "2024-02", 
                        "submitter": "NVIDIA"
                    }
                },
                "stable_diffusion": {
                    "H100": {
                        "queries_per_second": 2.5,
                        "latency_ms": 400,
                        "power_consumption_watts": 525,
                        "system_config": "1x H100 PCIe",
                        "submission_date": "2024-02",
                        "submitter": "NVIDIA"
                    },
                    "L40S": {
                        "queries_per_second": 1.8,
                        "latency_ms": 556,
                        "power_consumption_watts": 262,
                        "system_config": "1x L40S PCIe",
                        "submission_date": "2024-02",
                        "submitter": "NVIDIA"
                    }
                }
            }
        }
        
        return mlperf_data
    
    def save_data_to_files(self, gpu_specs: Dict, mlperf_data: Dict):
        """ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # GPU ì‚¬ì–‘ ë°ì´í„° ì €ì¥
        gpu_specs_file = os.path.join(self.raw_data_dir, f"gpu_specifications_{timestamp}.json")
        with open(gpu_specs_file, 'w', encoding='utf-8') as f:
            json.dump(gpu_specs, f, indent=2, ensure_ascii=False)
        
        # MLPerf ë°ì´í„° ì €ì¥  
        mlperf_file = os.path.join(self.raw_data_dir, f"mlperf_benchmarks_{timestamp}.json")
        with open(mlperf_file, 'w', encoding='utf-8') as f:
            json.dump(mlperf_data, f, indent=2, ensure_ascii=False)
        
        # CSV í˜•íƒœë¡œë„ ì €ì¥ (ë¶„ì„ ìš©ì´ì„±)
        self._save_gpu_specs_csv(gpu_specs, timestamp)
        self._save_mlperf_csv(mlperf_data, timestamp)
        
        print(f"âœ… ë°ì´í„° ì €ì¥ ì™„ë£Œ:")
        print(f"   - GPU ì‚¬ì–‘: {gpu_specs_file}")
        print(f"   - MLPerf: {mlperf_file}")
    
    def _save_gpu_specs_csv(self, gpu_specs: Dict, timestamp: str):
        """GPU ì‚¬ì–‘ì„ CSVë¡œ ì €ì¥"""
        csv_file = os.path.join(self.processed_data_dir, f"gpu_specs_{timestamp}.csv")
        
        # DataFrameìœ¼ë¡œ ë³€í™˜
        rows = []
        for gpu_model, specs in gpu_specs.items():
            row = {
                'gpu_model': gpu_model,
                'name': specs['name'],
                'architecture': specs['architecture'],
                'tdp_watts': specs['tdp_watts'],
                'cuda_cores': specs['cuda_cores'],
                'tensor_cores': specs['tensor_cores'],
                'memory_gb': specs['memory_size_gb'],
                'memory_bandwidth_gbps': specs['memory_bandwidth_gbps'],
                'release_date': specs['release_date'],
                'power_efficiency': specs['power_efficiency_tops_per_watt']
            }
            
            # AI ì„±ëŠ¥ ë°ì´í„° ì¶”ê°€
            for perf_type, value in specs['ai_performance'].items():
                row[f'ai_{perf_type}'] = value
                
            rows.append(row)
        
        df = pd.DataFrame(rows)
        df.to_csv(csv_file, index=False, encoding='utf-8')
        print(f"   - GPU CSV: {csv_file}")
    
    def _save_mlperf_csv(self, mlperf_data: Dict, timestamp: str):
        """MLPerf ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥"""
        csv_file = os.path.join(self.processed_data_dir, f"mlperf_{timestamp}.csv")
        
        rows = []
        for benchmark_suite, workloads in mlperf_data.items():
            for workload, gpu_results in workloads.items():
                for gpu_model, results in gpu_results.items():
                    row = {
                        'benchmark_suite': benchmark_suite,
                        'workload': workload,
                        'gpu_model': gpu_model,
                        'power_consumption_watts': results['power_consumption_watts'],
                        'system_config': results['system_config'],
                        'submission_date': results['submission_date'],
                        'submitter': results['submitter']
                    }
                    
                    # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ê°€ (workloadì— ë”°ë¼ ë‹¤ë¦„)
                    for key, value in results.items():
                        if key not in ['system_config', 'submission_date', 'submitter', 'power_consumption_watts']:
                            row[key] = value
                    
                    rows.append(row)
        
        df = pd.DataFrame(rows)
        df.to_csv(csv_file, index=False, encoding='utf-8')
        print(f"   - MLPerf CSV: {csv_file}")
    
    def run_collection(self):
        """ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰"""
        print("ğŸš€ NVIDIA GPU ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
        
        # GPU ì‚¬ì–‘ ìˆ˜ì§‘
        print("ğŸ“Š GPU ì‚¬ì–‘ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        gpu_specs = self.collect_gpu_specifications()
        print(f"   âœ… {len(gpu_specs)}ê°œ GPU ëª¨ë¸ ìˆ˜ì§‘ ì™„ë£Œ")
        
        # MLPerf ë²¤ì¹˜ë§ˆí¬ ìˆ˜ì§‘
        print("ğŸ† MLPerf ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        mlperf_data = self.collect_mlperf_benchmarks()
        print(f"   âœ… MLPerf ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
        
        # ë°ì´í„° ì €ì¥
        print("ğŸ’¾ ë°ì´í„° ì €ì¥ ì¤‘...")
        self.save_data_to_files(gpu_specs, mlperf_data)
        
        print("ğŸ‰ NVIDIA GPU ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        return gpu_specs, mlperf_data


if __name__ == "__main__":
    collector = NVIDIADataCollector()
    collector.run_collection()